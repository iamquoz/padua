{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "def get_text_from_pdf(pdf_url):\n",
    "\tresponse = requests.get(pdf_url, timeout=5, stream=True)\n",
    "\n",
    "\twith open('temp.pdf', 'wb') as f:\n",
    "\t\tf.write(response.content)\n",
    "\n",
    "\treader = PdfReader('temp.pdf')\n",
    "\ttext = ''.join([page.extract_text() for page in reader.pages])\n",
    "\n",
    "\tif (text == ''):\n",
    "\t\tprint('Fallback to ocr, path = ' + pdf_url)\n",
    "\t\timport ocrmypdf\n",
    "\n",
    "\t\tocrmypdf.ocr('temp.pdf', 'temp_ocr.pdf', language='rus', deskew=True)\n",
    "\t\treader = PdfReader('temp_ocr.pdf')\n",
    "\t\ttext = ''.join([page.extract_text() for page in reader.pages])\n",
    "\t\n",
    "\treturn text\n",
    "\n",
    "def save_pdf_to_txt(pdf_url):\n",
    "\ttext = get_text_from_pdf(pdf_url)\n",
    "\t\n",
    "\tpath = '/'.join(pdf_url.replace('https://', '').split('/')[:-1]) + '/' + pdf_url.split('/')[-1].replace('.pdf', '.txt')\n",
    "\n",
    "\tos.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\twith open(path, 'w') as f:\n",
    "\t\tf.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url, level = 0):\n",
    "\t\n",
    "\tprint(url, level)\n",
    "\n",
    "\ttry:\n",
    "\t\tpage = requests.get(url, timeout=5)\n",
    "\t\tsoup = BeautifulSoup(page.content, 'html.parser')\n",
    "\t\t\n",
    "\t\tfor pdf_link in soup.find_all('a', href=True):\n",
    "\t\t\tif pdf_link['href'].endswith('.pdf'):\n",
    "\t\t\t\tpdf_url = urljoin(url, pdf_link['href'])\n",
    "\t\t\t\tsave_pdf_to_txt(pdf_url)\n",
    "\n",
    "\t\tfor link in soup.find_all('a', href=True):\n",
    "\n",
    "\t\t\thref = link.get('href')\n",
    "\t\t\tif href is None:\n",
    "\t\t\t\tcontinue\n",
    "\t\n",
    "\t\t\telif level < 2 and '.' not in href:\n",
    "\t\t\t\tget_links(urljoin(url, href), level + 1)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pstu.ru/sveden/education/ 0\n",
      "Fallback to ocr, path = https://pstu.ru/files/file/Abitur/БУПы/2020/{04dfb049-eb61-4f47-975e-fe7a239b32ae}.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4b8c83412a4c31a92d8d56aff100f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_links(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
